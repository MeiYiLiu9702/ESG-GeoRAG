{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6eded0b2",
   "metadata": {},
   "source": [
    "# 本程式建議使用 colab 執行"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e676fdc",
   "metadata": {},
   "source": [
    "# 環境設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09ef42be",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 若執行環境為 google colab pro+請執行以下程序\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "## terminal 指令集，以下程序建議打開 colab pro+ 的 terminal 執行\n",
    "# !curl -fsSL https://ollama.com/install.sh | sh\n",
    "# !ollama serve &\n",
    "# !ollama pull llama3.1:8b\n",
    "# !pip install -U langchain langchain-community langchain-openai langchain-ollama tiktoken ragas sacrebleu faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d390e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "import hashlib\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List, Any, Optional, Union\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd12ad57",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "    OPENAI_AVAILABLE = True\n",
    "except ImportError:\n",
    "    OPENAI_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c003075",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Embedding 模型與 LLM 呼叫設定\n",
    "# emvbedding 模型，預設使用 ollama 執行\n",
    "embedding_model_init = OllamaEmbeddings(model=\"llama3.1:8b\")\n",
    "\n",
    "## llm 模型\n",
    "llm = ChatOllama(model=\"llama3.1:8b\", temperature=0.6, max_tokens=4096)\n",
    "\n",
    "# 若要使用 openai api 請取消下面兩行註解，並貼上自己的 api token\n",
    "# api_key=\"sk-proj--...\"\n",
    "# llm = ChatOpenAI(model=\"gpt-4o\", temperature=1, openai_api_key=api_key, max_tokens = 4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad693f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 向量資料庫路徑設定\n",
    "VECTORSTORE_ROOT_PATH = '/content/drive/MyDrive/03_碩班研究/llm_risk/data_0511/2_pdf_extract_db/ollama_300_char/vector_store'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df0a495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定執行結果的輸出資料夾路徑\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "\n",
    "output_dir = '/content/drive/MyDrive/03_碩班研究/llm_risk/data_0511/2_pdf_extract_db/0718_Report/chatGPT'\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f094a1",
   "metadata": {},
   "source": [
    "# 任務文字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e52bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 只傳遞給 Retriever 的 Query 文字\n",
    "query_task1 = \"\"\"\n",
    "Extract geographic information and location entities from text. Identify locations, company facilities, and addresses in text. Prompt for a text-based geographic information extraction system\n",
    "\"\"\"\n",
    "\n",
    "query_task2 = \"\"\"\n",
    "Extract extreme weather events and TCFD climate risks from text. Identify climate-related risks like floods, heatwaves, and TCFD disclosures. Tool for parsing corporate disclosures to find mentions of extreme weather.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325ba308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 傳給 LLM 的 prompt 內容\n",
    "TASK1_ZERO_SHOT = \"\"\"\n",
    "You are a geographic information extraction system. Your task is to identify and extract geographic entities from the given text.\n",
    "\n",
    "Extract the following types of geographic information:\n",
    "- Countries, cities, provinces, regions\n",
    "- Company facilities (plants, factories, offices, sites)\n",
    "- Addresses and specific locations\n",
    "\n",
    "Return your answer in the following JSON format ONLY:\n",
    "{\n",
    "  \"detection\": 1 or 0,\n",
    "  \"entities\": [\n",
    "    {\n",
    "      \"text\": \"extracted entity text\",\n",
    "      \"type\": \"location/facility/address\",\n",
    "      \"page\": page_number\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "Rules:\n",
    "- detection: 1 if ANY geographic entities found, 0 if NONE found\n",
    "- Only return the JSON format above, no other text\n",
    "- If no entities found, return: {\"detection\": 0, \"entities\": []}\n",
    "\"\"\"\n",
    "\n",
    "TASK1_ONE_SHOT = \"\"\"\n",
    "You are a geographic information extraction system. Your task is to identify and extract geographic entities from the given text.\n",
    "\n",
    "Extract the following types of geographic information:\n",
    "- Countries, cities, provinces, regions\n",
    "- Company facilities (plants, factories, offices, sites)\n",
    "- Addresses and specific locations\n",
    "\n",
    "Example:\n",
    "Input: \"Our headquarters is located in Taipei, Taiwan. We operate manufacturing plants in Suzhou, China and have a regional office in Singapore.\"\n",
    "Output: {\n",
    "  \"detection\": 1,\n",
    "  \"entities\": [\n",
    "    {\"text\": \"Taipei\", \"type\": \"location\", \"page\": 1},\n",
    "    {\"text\": \"Taiwan\", \"type\": \"location\", \"page\": 1},\n",
    "    {\"text\": \"Suzhou\", \"type\": \"location\", \"page\": 1},\n",
    "    {\"text\": \"China\", \"type\": \"location\", \"page\": 1},\n",
    "    {\"text\": \"Singapore\", \"type\": \"location\", \"page\": 1},\n",
    "    {\"text\": \"headquarters\", \"type\": \"facility\", \"page\": 1},\n",
    "    {\"text\": \"manufacturing plants\", \"type\": \"facility\", \"page\": 1},\n",
    "    {\"text\": \"regional office\", \"type\": \"facility\", \"page\": 1}\n",
    "  ]\n",
    "}\n",
    "\n",
    "Return your answer in the following JSON format ONLY:\n",
    "{\n",
    "  \"detection\": 1 or 0,\n",
    "  \"entities\": [\n",
    "    {\n",
    "      \"text\": \"extracted entity text\",\n",
    "      \"type\": \"location/facility/address\",\n",
    "      \"page\": page_number\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "Rules:\n",
    "- detection: 1 if ANY geographic entities found, 0 if NONE found\n",
    "- Only return the JSON format above, no other text\n",
    "- If no entities found, return: {\"detection\": 0, \"entities\": []}\n",
    "\"\"\"\n",
    "\n",
    "TASK2_ZERO_SHOT = \"\"\"\n",
    "You are a climate risk disclosure analyzer. Your task is to identify extreme weather events and TCFD-related climate risk information.\n",
    "\n",
    "Identify the following extreme weather events:\n",
    "- Heat waves, cold waves, typhoons, floods, droughts, landslides, sea level rise\n",
    "\n",
    "Also identify TCFD disclosure elements:\n",
    "- Risk identification, risk assessment, financial impact, response strategies\n",
    "\n",
    "Return your answer in the following JSON format ONLY:\n",
    "{\n",
    "  \"detection\": 1 or 0,\n",
    "  \"events\": [\n",
    "    {\n",
    "      \"text\": \"extracted event/disclosure text\",\n",
    "      \"type\": \"weather_event/tcfd_element\",\n",
    "      \"category\": \"specific category\",\n",
    "      \"page\": page_number\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "Rules:\n",
    "- detection: 1 if ANY extreme weather events or TCFD elements found, 0 if NONE found\n",
    "- Only return the JSON format above, no other text\n",
    "- If nothing found, return: {\"detection\": 0, \"events\": []}\n",
    "\"\"\"\n",
    "\n",
    "TASK2_ONE_SHOT = \"\"\"\n",
    "You are a climate risk disclosure analyzer. Your task is to identify extreme weather events and TCFD-related climate risk information.\n",
    "\n",
    "Identify the following extreme weather events:\n",
    "- Heat waves, cold waves, typhoons, floods, droughts, landslides, sea level rise\n",
    "\n",
    "Also identify TCFD disclosure elements:\n",
    "- Risk identification, risk assessment, financial impact, response strategies\n",
    "\n",
    "Example:\n",
    "Input: \"Climate change poses significant risks to our operations. Extreme weather events such as typhoons and floods may disrupt our supply chain. We assess physical risks including heat waves that could affect our facilities.\"\n",
    "Output: {\n",
    "  \"detection\": 1,\n",
    "  \"events\": [\n",
    "    {\"text\": \"typhoons\", \"type\": \"weather_event\", \"category\": \"typhoon\", \"page\": 1},\n",
    "    {\"text\": \"floods\", \"type\": \"weather_event\", \"category\": \"flood\", \"page\": 1},\n",
    "    {\"text\": \"heat waves\", \"type\": \"weather_event\", \"category\": \"heat_wave\", \"page\": 1},\n",
    "    {\"text\": \"assess physical risks\", \"type\": \"tcfd_element\", \"category\": \"risk_assessment\", \"page\": 1},\n",
    "    {\"text\": \"risks to our operations\", \"type\": \"tcfd_element\", \"category\": \"risk_identification\", \"page\": 1}\n",
    "  ]\n",
    "}\n",
    "\n",
    "Return your answer in the following JSON format ONLY:\n",
    "{\n",
    "  \"detection\": 1 or 0,\n",
    "  \"events\": [\n",
    "    {\n",
    "      \"text\": \"extracted event/disclosure text\",\n",
    "      \"type\": \"weather_event/tcfd_element\",\n",
    "      \"category\": \"specific category\",\n",
    "      \"page\": page_number\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "Rules:\n",
    "- detection: 1 if ANY extreme weather events or TCFD elements found, 0 if NONE found\n",
    "- Only return the JSON format above, no other text\n",
    "- If nothing found, return: {\"detection\": 0, \"events\": []}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b734236c",
   "metadata": {},
   "source": [
    "# 功能函數"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7525d327",
   "metadata": {},
   "source": [
    "## 取得檔案、資料排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd697c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def natural_sort(s):\n",
    "    import re\n",
    "    return [int(text) if text.isdigit() else text.lower()\n",
    "            for text in re.split('([0-9]+)', s)]\n",
    "\n",
    "def get_pdf_list(vectorstore_path):\n",
    "    pdf_vectorstore_list = sorted(os.listdir(vectorstore_path), key=natural_sort)\n",
    "    if '.DS_Store' in pdf_vectorstore_list:\n",
    "        pdf_vectorstore_list.remove('.DS_Store')\n",
    "    return pdf_vectorstore_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcda986c",
   "metadata": {},
   "source": [
    "## RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61d71e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from operator import itemgetter\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def rag_qa_pipeline(pdfname, question, llm, limited_token=4096, query=None, QUERY_OPT=False, vectorstore_path = VECTORSTORE_ROOT_PATH, embedding_model = embedding_model_init, search_kwargs_init = 20):\n",
    "  # llm 跟 embedding 都是在外部初始化匯入\n",
    "\n",
    "  Sys_Template = f\"\"\"\n",
    "  You are a professional information extraction assistant.\n",
    "  Please analyze the provided context and answer the question based ONLY on the given context.\n",
    "  Context:\n",
    "  {{context}}\n",
    "\n",
    "  Question:\n",
    "  {{question}}\n",
    "\n",
    "  Answer (in English, JSON format only)\n",
    "  \"\"\"\n",
    "\n",
    "      # 合併多個答案的模板\n",
    "  Merge_Template = f\"\"\"\n",
    "  You are a professional information synthesis assistant.\n",
    "  Please synthesize the following partial answers into one comprehensive, coherent answer.\n",
    "\n",
    "  Partial Answers:\n",
    "  {{partial_answers}}\n",
    "\n",
    "  Original Question:\n",
    "  {{question}}\n",
    "\n",
    "  Final Answer (in English, JSON format only):\n",
    "  \"\"\"\n",
    "  # 匯入向量資料庫\n",
    "  vectorstore = FAISS.load_local(\n",
    "      folder_path = os.path.join(vectorstore_path, pdfname),\n",
    "      embeddings = embedding_model,\n",
    "      allow_dangerous_deserialization=True\n",
    "  )\n",
    "\n",
    "  # 設定檢索器\n",
    "  retriever = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": search_kwargs_init})\n",
    "\n",
    "  # 建立問答執行流程\n",
    "\n",
    "  if QUERY_OPT == True:\n",
    "    print(f'{pdfname} 執行加入額外索引問答的方式')\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(Sys_Template)\n",
    "    setup_and_retrieval = RunnableParallel(\n",
    "        # 研究設計編號 2 的執行流程在這裡\n",
    "        # !!! 這一行的 context 就是獨立丟一個 query 給 retriever 進行查詢。\n",
    "        context = (itemgetter(\"retriever_query\") | retriever),\n",
    "        question = itemgetter(\"question\")\n",
    "    )\n",
    "    setup_result = setup_and_retrieval.invoke({\n",
    "        \"retriever_query\":query,\n",
    "        \"question\":question\n",
    "    })\n",
    "  else:\n",
    "    prompt = ChatPromptTemplate.from_template(Sys_Template)\n",
    "    # 研究設計編號 1 的執行流程在這裡\n",
    "    setup_and_retrieval = RunnableParallel(\n",
    "        {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    )\n",
    "    setup_result = setup_and_retrieval.invoke(question)\n",
    "\n",
    "  # 針對輸入 llm 含有完整上下文的 prompt 計算 token 長度\n",
    "\n",
    "  formatted_prompt = prompt.format(context = setup_result[\"context\"], question= setup_result[\"question\"])\n",
    "  prompt_tokens = llm.get_num_tokens(formatted_prompt)\n",
    "  if prompt_tokens > limited_token:\n",
    "    print(f\"Token 數量 ({prompt_tokens}) 超出限制 ({limited_token})，開始分批處理\")\n",
    "    # 分批處理 context\n",
    "    context_docs = setup_result[\"context\"]\n",
    "    question_text = setup_result[\"question\"]\n",
    "\n",
    "    # 計算基礎 prompt 的 token 數量（不包含 context）\n",
    "    base_prompt = prompt.format(context=\"\", question=question_text)\n",
    "    base_tokens = llm.get_num_tokens(base_prompt)\n",
    "\n",
    "    # 計算每批 context 可用的 token 數量\n",
    "    available_tokens = limited_token - base_tokens - 100\n",
    "\n",
    "    # 將 context 分批\n",
    "    batches = []\n",
    "    current_batch = []\n",
    "    current_batch_tokens = 0\n",
    "\n",
    "    for doc in context_docs:\n",
    "      doc_text = doc.page_content if hasattr(doc, 'page_content') else str(doc)\n",
    "      doc_tokens = llm.get_num_tokens(doc_text)\n",
    "      # 檢查是否需要開始新批次\n",
    "      if current_batch_tokens + doc_tokens > available_tokens and current_batch:\n",
    "        batches.append(current_batch)\n",
    "        current_batch = [doc]\n",
    "        current_batch_tokens = doc_tokens\n",
    "      else:\n",
    "        current_batch.append(doc)\n",
    "        current_batch_tokens += doc_tokens\n",
    "\n",
    "    if current_batch:\n",
    "      batches.append(current_batch)\n",
    "\n",
    "    print(f\"將 context 分為 {len(batches)} 批次處理\")\n",
    "\n",
    "    partial_answers = []\n",
    "    for i, batch in enumerate(batches):\n",
    "      # print(f\"處理第 {i+1}/{len(batches)} 批次\")\n",
    "      # 建立本批次的 qa_chain\n",
    "            batch_setup_and_retrieval = RunnableParallel(\n",
    "                context=lambda x: batch,\n",
    "                question=lambda x: question_text\n",
    "            )\n",
    "\n",
    "            batch_qa_chain = batch_setup_and_retrieval.assign(\n",
    "                answer=(lambda x: {\"context\": x[\"context\"], \"question\": x[\"question\"]}) | prompt | llm | StrOutputParser()\n",
    "            )\n",
    "\n",
    "            # 執行本批次問答\n",
    "            batch_response = batch_qa_chain.invoke({})\n",
    "            partial_answers.append(batch_response[\"answer\"])\n",
    "\n",
    "    # 合併所有部分答案\n",
    "    print(\"合併所有部分答案\")\n",
    "    merge_prompt = ChatPromptTemplate.from_template(Merge_Template)\n",
    "    merge_chain = merge_prompt | llm | StrOutputParser()\n",
    "\n",
    "    final_answer = merge_chain.invoke({\n",
    "        \"partial_answers\": \"\\n\\n\".join([f\"Answer {i+1}: {ans}\" for i, ans in enumerate(partial_answers)]),\n",
    "        \"question\": question_text\n",
    "    })\n",
    "\n",
    "    # 組成最終 response\n",
    "    response = {\n",
    "        \"context\": setup_result[\"context\"],\n",
    "        \"question\": question_text,\n",
    "        \"answer\": final_answer\n",
    "    }\n",
    "\n",
    "\n",
    "  else:\n",
    "    print(f\"Token 數量 ({prompt_tokens}) 在限制內，使用正常流程\")\n",
    "    qa_chain = setup_and_retrieval.assign(answer = (lambda x: {\"context\": x[\"context\"], \"question\": x[\"question\"]}) | prompt | llm | StrOutputParser())\n",
    "    # 執行問答\n",
    "    if QUERY_OPT == True:\n",
    "      response = qa_chain.invoke({\n",
    "          \"retriever_query\": query,\n",
    "          \"question\":question\n",
    "      })\n",
    "    else:\n",
    "      response = qa_chain.invoke(question)\n",
    "\n",
    "  print(f'{pdfname}問答完成')\n",
    "\n",
    "  return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71786d3",
   "metadata": {},
   "source": [
    "## 解析 RAG 回傳結果（Response）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac7ddc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm 回傳的 json 格式\n",
    "def safe_json_parse(text):\n",
    "    \"\"\"Safely parse JSON response from LLM\"\"\"\n",
    "    try:\n",
    "        # Clean the text - remove any markdown formatting or extra text\n",
    "        text = text.strip()\n",
    "        if text.startswith('```json'):\n",
    "            text = text[7:]\n",
    "        if text.endswith('```'):\n",
    "            text = text[:-3]\n",
    "        text = text.strip()\n",
    "\n",
    "        # Find JSON object boundaries\n",
    "        start_idx = text.find('{')\n",
    "        end_idx = text.rfind('}') + 1\n",
    "\n",
    "        if start_idx != -1 and end_idx > start_idx:\n",
    "            json_text = text[start_idx:end_idx]\n",
    "            return json.loads(json_text)\n",
    "        else:\n",
    "            return {\"detection\": 0, \"entities\": [], \"events\": []}\n",
    "    except:\n",
    "        return {\"detection\": 0, \"entities\": [], \"events\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a292a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 快速計算 Performance\n",
    "def calculate_performance_metrics(ground_truth, predicted):\n",
    "    \"\"\"\n",
    "    Calculate TP, FP, FN, TN for evaluation\n",
    "    \"\"\"\n",
    "    # Convert to binary\n",
    "    gt_binary = 1 if ground_truth == 1 else 0\n",
    "    pred_binary = 1 if predicted == 1 else 0\n",
    "\n",
    "    if gt_binary == 1 and pred_binary == 1:\n",
    "        return 'TP'\n",
    "    elif gt_binary == 0 and pred_binary == 1:\n",
    "        return 'FP'\n",
    "    elif gt_binary == 1 and pred_binary == 0:\n",
    "        return 'FN'\n",
    "    else:  # gt_binary == 0 and pred_binary == 0\n",
    "        return 'TN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd30d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解析 rag_qa_pipeline 回傳內容的執行程式\n",
    "# 會完整的匯出所有 retriever 提供的 context ，並且辨別 LLM 回傳的資料哪一筆其實是幻覺\n",
    "import re\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "def decode_response(response, task_name):\n",
    "    contexts = response['context']\n",
    "    llm_answer = response['answer']\n",
    "    parsed_result = safe_json_parse(llm_answer)\n",
    "\n",
    "    pdf_name = f\"{contexts[0].metadata['company']}_{contexts[0].metadata['year']}\"\n",
    "\n",
    "\n",
    "    retrieval_records = []\n",
    "    for doc_idx, doc in enumerate(contexts):\n",
    "      pdfname = pdf_name\n",
    "      query_id = f\"{task_name}_{pdfname}\"\n",
    "      content_hash = hashlib.md5(doc.page_content.encode()).hexdigest()[:8]\n",
    "      doc_id = f\"{doc.metadata.get('company', 'unknown')}_{doc.metadata.get('year', 'unknown')}_{doc.metadata.get('page', 'unknown')}_{content_hash}\"\n",
    "\n",
    "      retrieval_record = {\n",
    "          'query_id': query_id,\n",
    "          'doc_index': doc_idx,\n",
    "          'doc_id': doc_id,\n",
    "          'content_hash': content_hash,\n",
    "          'pdf_name': pdf_name,\n",
    "          'company': doc.metadata.get('company', 'unknown'),\n",
    "          'year': doc.metadata.get('year', 'unknown'),\n",
    "          'page': doc.metadata.get('page', 'unknown'),\n",
    "          'standard': doc.metadata.get('standard', 'unknown'),\n",
    "          'label': doc.metadata.get('label', 'unknown'),\n",
    "          'geog_ans_ground_truth': doc.metadata.get('geog_ans', None),\n",
    "          'org_ans_ground_truth': doc.metadata.get('org_ans', None),\n",
    "          'events_ans_ground_truth': doc.metadata.get('event_ans', None),\n",
    "          'content': doc.page_content,\n",
    "          'content_length': len(doc.page_content)\n",
    "      }\n",
    "      retrieval_records.append(retrieval_record)\n",
    "\n",
    "    doc_metadata = []\n",
    "    if task_name == 'Task1_ZeroShot' or task_name == 'Task1_OneShot':\n",
    "        entity_key = 'entities'\n",
    "    else:\n",
    "        entity_key = 'events'\n",
    "\n",
    "    detection = parsed_result.get('detection', 0)\n",
    "    entities_or_events = parsed_result.get(entity_key, [])\n",
    "\n",
    "    page_list = [doc.metadata['page'] for doc in contexts]\n",
    "    text_list = [doc.page_content for doc in contexts]\n",
    "\n",
    "    def preprocess_text(text: str) -> str:\n",
    "        text = re.sub(r'\\s+', ' ', text.strip())\n",
    "        text = text.lower()\n",
    "        return text\n",
    "\n",
    "    def find_exact_substring(llm_text: str, original_text: str) -> Tuple[bool, str]:\n",
    "        llm_processed = preprocess_text(llm_text)\n",
    "        original_processed = preprocess_text(original_text)\n",
    "        if llm_processed in original_processed:\n",
    "            return True\n",
    "        if original_processed in llm_processed:\n",
    "            return True\n",
    "        for i in range(len(llm_processed) - 20):\n",
    "            substring = llm_processed[i:i+20]\n",
    "            if substring in original_processed:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    # 為每個 context 處理\n",
    "    for context in contexts:\n",
    "        metadata = context.metadata\n",
    "\n",
    "        # 計算 ground_truth\n",
    "        if task_name == 'Task1_ZeroShot' or task_name == 'Task1_OneShot':\n",
    "            if metadata['geog_ans'] == 1 or metadata['org_ans'] == 1:\n",
    "                ground_truth = 1\n",
    "            else:\n",
    "                ground_truth = 0\n",
    "        else:\n",
    "            ground_truth = metadata['event_ans']\n",
    "\n",
    "        # 初始化基本 doc_info\n",
    "        doc_info = {\n",
    "            'pdf_name': pdf_name,\n",
    "            'company': metadata['company'],\n",
    "            'year': metadata['year'],\n",
    "            'standard': metadata['standard'],\n",
    "            'page': metadata['page'],\n",
    "            'label': metadata['label'],\n",
    "            \"llm_page\": 0,\n",
    "            'llm_ans': 0,\n",
    "            'llm_detection': detection,  # 統一設置 detection\n",
    "            'ground_truth': ground_truth,\n",
    "            'performance': None,\n",
    "            \"results\": '',\n",
    "            'type': \"\",\n",
    "            'num_retrieved_docs': len(contexts),\n",
    "            'raw_llm_response': llm_answer,\n",
    "            'task_type': task_name\n",
    "        }\n",
    "\n",
    "        # 標記是否找到匹配\n",
    "        found_match = False\n",
    "\n",
    "        if entities_or_events:\n",
    "            # print('開始對答案')\n",
    "\n",
    "            for item in entities_or_events:\n",
    "                page_num = item.get('page', 'unknown')\n",
    "\n",
    "                # 情況1: 頁數完全匹配\n",
    "                if page_num == metadata['page']:\n",
    "                    # print('成功找出對應頁數')\n",
    "                    predicted = 1 if item.get('text', '').strip() else 0\n",
    "                    performance = calculate_performance_metrics(ground_truth, predicted) if ground_truth is not None else 'Unknown'\n",
    "\n",
    "                    doc_info.update({\n",
    "                        'llm_page': page_num,\n",
    "                        'llm_ans': predicted,\n",
    "                        'performance': performance,\n",
    "                        'results': item.get('text', ''),\n",
    "                        'type': item.get('type', '')\n",
    "                    })\n",
    "                    found_match = True\n",
    "                    break  # 找到匹配就跳出，避免重複\n",
    "\n",
    "                # 情況2: 頁數未知，嘗試文本匹配\n",
    "                elif page_num == 'unknown':\n",
    "                    # print('檢查文本匹配')\n",
    "                    if find_exact_substring(item.get('text', ''), context.page_content):\n",
    "                        # print('通過文本匹配找到對應')\n",
    "                        predicted = 1 if item.get('text', '').strip() else 0\n",
    "                        performance = calculate_performance_metrics(ground_truth, predicted) if ground_truth is not None else 'Unknown'\n",
    "\n",
    "                        doc_info.update({\n",
    "                            'llm_page': 'unknown',\n",
    "                            'llm_ans': predicted,\n",
    "                            'performance': performance,\n",
    "                            'results': item.get('text', ''),\n",
    "                            'type': item.get('type', '')\n",
    "                        })\n",
    "                        found_match = True\n",
    "                        break  # 找到匹配就跳出\n",
    "\n",
    "            # 如果沒有找到匹配，檢查是否有幻覺情況需要記錄\n",
    "            if not found_match:\n",
    "                # 檢查是否有針對不存在頁數的回答（幻覺）\n",
    "                for item in entities_or_events:\n",
    "                    page_num = item.get('page', 'unknown')\n",
    "                    if page_num != 'unknown' and page_num not in page_list:\n",
    "                        # print('檢測到幻覺，但不為當前 context 創建記錄')\n",
    "                        # 不在這裡處理幻覺，避免重複\n",
    "                        pass\n",
    "\n",
    "                # 沒有找到任何匹配，使用預設值\n",
    "                # print('本輪 metadata 沒有跟 llm 輸出對應')\n",
    "                performance = calculate_performance_metrics(ground_truth, 0) if ground_truth is not None else 'Unknown'\n",
    "                doc_info['performance'] = performance\n",
    "\n",
    "        else:\n",
    "            # print('跳過對答案 - 沒有 entities_or_events')\n",
    "            performance = calculate_performance_metrics(ground_truth, 0) if ground_truth is not None else 'Unknown'\n",
    "            doc_info['performance'] = performance\n",
    "\n",
    "        # 每個 context 只加入一筆記錄\n",
    "        doc_metadata.append(doc_info.copy())  # 使用 copy() 避免引用問題\n",
    "\n",
    "    # 單獨處理幻覺情況（避免重複）\n",
    "    hallucination_items = []\n",
    "    for item in entities_or_events:\n",
    "        page_num = item.get('page', 'unknown')\n",
    "        if page_num != 'unknown' and page_num not in page_list:\n",
    "            hallucination_items.append(item)\n",
    "\n",
    "    # 為幻覺項目創建單獨記錄\n",
    "    for item in hallucination_items:\n",
    "        print('處理幻覺項目')\n",
    "        first_metadata = contexts[0].metadata  # 使用第一個 context 的基本信息\n",
    "\n",
    "        doc_info = {\n",
    "            'pdf_name': pdf_name,\n",
    "            'company': first_metadata['company'],\n",
    "            'year': first_metadata['year'],\n",
    "            'standard': \"\",\n",
    "            'page': \"\",\n",
    "            'label': \"\",\n",
    "            \"llm_page\": item.get('page', 'unknown'),\n",
    "            'llm_ans': 1 if item.get('text', '').strip() else 0,\n",
    "            'llm_detection': detection,\n",
    "            'ground_truth': 0,  # 幻覺情況 ground_truth 為 0\n",
    "            'performance': calculate_performance_metrics(0, 1),\n",
    "            \"results\": item.get('text', ''),\n",
    "            'type': item.get('type', ''),\n",
    "            'num_retrieved_docs': len(contexts),\n",
    "            'raw_llm_response': llm_answer,\n",
    "            'task_type': task_name\n",
    "        }\n",
    "        doc_metadata.append(doc_info)\n",
    "\n",
    "    return doc_metadata, retrieval_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef29ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合併本輪問答問題所有取得的 context 內容\n",
    "def create_lookup_index(retrieval_records):\n",
    "    \"\"\"\n",
    "    Create lookup indices for easy querying\n",
    "    \"\"\"\n",
    "    # Create DataFrame for easy querying\n",
    "    retrieval_df = pd.DataFrame(retrieval_records)\n",
    "\n",
    "    # Create various lookup indices\n",
    "    lookup_indices = {\n",
    "        'by_query_id': retrieval_df.groupby('query_id').apply(lambda x: x.to_dict('records')).to_dict(),\n",
    "        'by_doc_id': retrieval_df.groupby('doc_id').apply(lambda x: x.to_dict('records')).to_dict(),\n",
    "        'by_pdf_name': retrieval_df.groupby('pdf_name').apply(lambda x: x.to_dict('records')).to_dict(),\n",
    "        'by_company_year': retrieval_df.groupby(['company', 'year']).apply(lambda x: x.to_dict('records')).to_dict(),\n",
    "        'by_performance': retrieval_df.groupby(['geog_ans_ground_truth', 'events_ans_ground_truth']).apply(lambda x: x.to_dict('records')).to_dict()\n",
    "    }\n",
    "\n",
    "    return lookup_indices, retrieval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccadab4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將前面函數計算的 Performance 轉換成 F1-Score 需要的各項參數並進行計算\n",
    "def analyze_performance_detailed(results_df, retrieval_df, task_name):\n",
    "    \"\"\"\n",
    "    Create detailed performance analysis\n",
    "    \"\"\"\n",
    "    analysis = {}\n",
    "\n",
    "    # Overall performance metrics\n",
    "    if 'performance' in results_df.columns:\n",
    "        performance_counts = results_df['performance'].value_counts()\n",
    "        total = len(results_df)\n",
    "\n",
    "        analysis['confusion_matrix'] = {\n",
    "            'TP': performance_counts.get('TP', 0),\n",
    "            'FP': performance_counts.get('FP', 0),\n",
    "            'FN': performance_counts.get('FN', 0),\n",
    "            'TN': performance_counts.get('TN', 0)\n",
    "        }\n",
    "\n",
    "        # Calculate metrics\n",
    "        tp = performance_counts.get('TP', 0)\n",
    "        fp = performance_counts.get('FP', 0)\n",
    "        fn = performance_counts.get('FN', 0)\n",
    "        tn = performance_counts.get('TN', 0)\n",
    "\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        accuracy = (tp + tn) / total if total > 0 else 0\n",
    "\n",
    "        analysis['metrics'] = {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'accuracy': accuracy\n",
    "        }\n",
    "\n",
    "    # Performance by company\n",
    "    if 'company' in results_df.columns and 'performance' in results_df.columns:\n",
    "        company_performance = results_df.groupby('company')['performance'].value_counts().unstack(fill_value=0)\n",
    "        analysis['by_company'] = company_performance.to_dict()\n",
    "\n",
    "    # Performance by standard\n",
    "    if 'standard' in results_df.columns and 'performance' in results_df.columns:\n",
    "        standard_performance = results_df.groupby('standard')['performance'].value_counts().unstack(fill_value=0)\n",
    "        analysis['by_standard'] = standard_performance.to_dict()\n",
    "\n",
    "    return analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73abebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================== LOOKUP AND QUERY FUNCTIONS ===================\n",
    "def lookup_retrieval_by_query_id(lookup_indices, query_id):\n",
    "    \"\"\"查詢特定 query_id 的檢索結果\"\"\"\n",
    "    return lookup_indices['by_query_id'].get(query_id, [])\n",
    "\n",
    "def lookup_retrieval_by_pdf_name(lookup_indices, pdf_name):\n",
    "    \"\"\"查詢特定PDF的所有檢索結果\"\"\"\n",
    "    return lookup_indices['by_pdf_name'].get(pdf_name, [])\n",
    "\n",
    "def lookup_retrieval_by_doc_id(lookup_indices, doc_id):\n",
    "    \"\"\"根據文件ID查詢檢索記錄\"\"\"\n",
    "    return lookup_indices['by_doc_id'].get(doc_id, [])\n",
    "\n",
    "def find_documents_with_ground_truth(retrieval_df, geog_ans=None, events_ans=None):\n",
    "    \"\"\"尋找有特定ground truth標籤的文件\"\"\"\n",
    "    query = retrieval_df\n",
    "    if geog_ans is not None:\n",
    "        query = query[query['geog_ans_ground_truth'] == geog_ans]\n",
    "    if events_ans is not None:\n",
    "        query = query[query['events_ans_ground_truth'] == events_ans]\n",
    "    return query\n",
    "\n",
    "def cross_reference_results(results_df, retrieval_df, query_id):\n",
    "    \"\"\"交叉參考特定查詢的結果和檢索記錄\"\"\"\n",
    "    result_records = results_df[results_df['query_id'] == query_id]\n",
    "    retrieval_records = retrieval_df[retrieval_df['query_id'] == query_id]\n",
    "\n",
    "    return {\n",
    "        'llm_results': result_records.to_dict('records'),\n",
    "        'retrieved_documents': retrieval_records.to_dict('records'),\n",
    "        'summary': {\n",
    "            'num_results': len(result_records),\n",
    "            'num_retrieved_docs': len(retrieval_records),\n",
    "            'ground_truth_distribution': retrieval_records.groupby(['geog_ans_ground_truth', 'events_ans_ground_truth']).size().to_dict()\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12aa9b74",
   "metadata": {},
   "source": [
    "# 主要執行程序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e58e474",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = {\n",
    "        \"Task1_ZeroShot\": TASK1_ZERO_SHOT,\n",
    "        \"Task1_OneShot\": TASK1_ONE_SHOT,\n",
    "        \"Task2_ZeroShot\": TASK2_ZERO_SHOT,\n",
    "        \"Task2_OneShot\": TASK2_ONE_SHOT\n",
    "    }\n",
    "\n",
    "# queries = {\n",
    "#     'Task1_query': query_task1,\n",
    "#     'Task2_query':query_task2\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1515745",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359c4225",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_list = get_pdf_list(VECTORSTORE_ROOT_PATH)\n",
    "print(f\"Found {len(pdf_list)} PDF folders to process\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea16143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 儲存本輪執行結果\n",
    "all_results = {}\n",
    "all_retrieval_records = []\n",
    "all_lookup_indices = {}\n",
    "\n",
    "for task_name, task_prompt in tasks.items():\n",
    "  task_type = task_name.split('_')[0]\n",
    "  print(task_type)\n",
    "  print(f\"\\n{'-'*40}\")\n",
    "  print(f\"Running {task_name}\")\n",
    "  print(f\"{'-'*40}\")\n",
    "  tmp_results = []\n",
    "  tmp_retrieval_records = []\n",
    "  tmp_lookup_indices = []\n",
    "  for pdf_name in tqdm(pdf_list):\n",
    "    # 如果需要額外傳入 query ，請調整 rag_qa_pipeline 的參數\n",
    "    response = rag_qa_pipeline(pdf_name, task_prompt, llm)\n",
    "    results, retrieval_records = decode_response(response, task_name)\n",
    "    tmp_results.append(results)\n",
    "    tmp_retrieval_records.extend(retrieval_records)\n",
    "  tmp_results = [item for sublist in tmp_results for item in sublist]\n",
    "  all_results[task_name] = tmp_results\n",
    "  all_retrieval_records.extend(tmp_retrieval_records)\n",
    "\n",
    "  # 轉換成 pandas_df\n",
    "  df = pd.DataFrame(tmp_results)\n",
    "  all_results[f\"{task_name}_DataFrame\"] = df\n",
    "\n",
    "  # 設定輸出路徑\n",
    "  output_path = os.path.join(output_dir, f\"{task_name}_results_{timestamp}.xlsx\")\n",
    "  with pd.ExcelWriter(output_path, engine='openpyxl') as writer:\n",
    "    df.to_excel(writer, sheet_name='Results', index=False)\n",
    "\n",
    "    task_retrieval_df = pd.DataFrame([r for r in tmp_retrieval_records])\n",
    "    task_retrieval_df.to_excel(writer, sheet_name='Retrieval_Records', index=False)\n",
    "\n",
    "    if not df.empty:\n",
    "      analysis = analyze_performance_detailed(df, task_retrieval_df, task_name)\n",
    "      if 'confusion_matrix' in analysis:\n",
    "        cm_df = pd.DataFrame([analysis['confusion_matrix']])\n",
    "        cm_df.to_excel(writer, sheet_name='Confusion_Matrix', index=False)\n",
    "      if 'metrics' in analysis:\n",
    "        metrics_df = pd.DataFrame([analysis['metrics']])\n",
    "        metrics_df.to_excel(writer, sheet_name='Performance_Metrics', index=False)\n",
    "\n",
    "  print(f\"Saved {task_name} detailed results to: {output_path}\")\n",
    "\n",
    "  if not df.empty:\n",
    "    total_docs = len(df)\n",
    "    detected_docs = len(df[df['llm_detection'] == 1])\n",
    "    found_entities = len(df[df['llm_ans'] == 1])\n",
    "    print(f\"Summary for {task_name}:\")\n",
    "    print(f\"  - Total documents processed: {total_docs}\")\n",
    "    print(f\"  - Documents with detection: {detected_docs}\")\n",
    "    print(f\"  - Entities/Events found: {found_entities}\")\n",
    "    print(f\"  - Detection rate: {detected_docs/total_docs*100:.2f}%\")\n",
    "  if 'performance' in df.columns:\n",
    "    perf_counts = df['performance'].value_counts()\n",
    "    print(f\"  - Performance breakdown:\")\n",
    "    for perf_type, count in perf_counts.items():\n",
    "      print(f\"    {perf_type}: {count} ({count/total_docs*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a86364c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 輸出匯總報表以及摘要\n",
    "all_retrieval_df = pd.DataFrame(all_retrieval_records)\n",
    "lookup_indices, retrieval_df = create_lookup_index(all_retrieval_records)\n",
    "comprehensive_data = {\n",
    "        'task_results': all_results,\n",
    "        'retrieval_records': all_retrieval_records,\n",
    "        'lookup_indices': lookup_indices,\n",
    "        'retrieval_dataframe': all_retrieval_df\n",
    "    }\n",
    "\n",
    "pickle_path = os.path.join(output_dir, f'comprehensive_rag_evaluation_results{timestamp}.pkl')\n",
    "with open(pickle_path, 'wb') as f:\n",
    "  pickle.dump(comprehensive_data, f)\n",
    "\n",
    "retrieval_db_path = os.path.join(output_dir, f'retrieval_database{timestamp}.xlsx')\n",
    "with pd.ExcelWriter(retrieval_db_path, engine='openpyxl') as writer:\n",
    "  all_retrieval_df.to_excel(writer, sheet_name='All_Retrievals', index=False)\n",
    "  pdf_summary = all_retrieval_df.groupby(['company','year']).agg({\n",
    "            'query_id': 'count',\n",
    "            'geog_ans_ground_truth': lambda x: (x == 1).sum(),\n",
    "            'events_ans_ground_truth': lambda x: (x == 1).sum()\n",
    "        }).rename(columns={\n",
    "            'query_id': 'total_retrievals',\n",
    "            'geog_ans_ground_truth': 'geog_positive_docs',\n",
    "            'events_ans_ground_truth': 'events_positive_docs'\n",
    "        })\n",
    "  pdf_summary.to_excel(writer, sheet_name='PDF_Summary')\n",
    "\n",
    "  company_summary = all_retrieval_df.groupby('company').agg({\n",
    "            'query_id': 'count',\n",
    "            'geog_ans_ground_truth': lambda x: (x == 1).sum(),\n",
    "            'events_ans_ground_truth': lambda x: (x == 1).sum()\n",
    "        }).rename(columns={\n",
    "            'query_id': 'total_retrievals',\n",
    "            'geog_ans_ground_truth': 'geog_positive_docs',\n",
    "            'events_ans_ground_truth': 'events_positive_docs'\n",
    "        })\n",
    "  company_summary.to_excel(writer, sheet_name='Company_Summary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691129b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_data = []\n",
    "for task_name in tasks.keys():\n",
    "  if f\"{task_name}_DataFrame\" in all_results:\n",
    "    df = all_results[f\"{task_name}_DataFrame\"]\n",
    "    if not df.empty:\n",
    "      total = len(df)\n",
    "      detected = len(df[df['llm_detection'] == 1])\n",
    "      found = len(df[df['llm_ans'] == 1])\n",
    "      summary_row = {\n",
    "                    'Task': task_name,\n",
    "                    'Total_Documents': total,\n",
    "                    'Documents_With_Detection': detected,\n",
    "                    'Entities_Found': found,\n",
    "                    'Detection_Rate': detected/total*100 if total > 0 else 0,\n",
    "                    'Success_Rate': found/total*100 if total > 0 else 0\n",
    "                }\n",
    "      if 'performance' in df.columns:\n",
    "                    perf_counts = df['performance'].value_counts()\n",
    "                    summary_row.update({\n",
    "                        'TP': perf_counts.get('TP', 0),\n",
    "                        'FP': perf_counts.get('FP', 0),\n",
    "                        'FN': perf_counts.get('FN', 0),\n",
    "                        'TN': perf_counts.get('TN', 0),\n",
    "                        'Precision': perf_counts.get('TP', 0) / (perf_counts.get('TP', 0) + perf_counts.get('FP', 0)) if (perf_counts.get('TP', 0) + perf_counts.get('FP', 0)) > 0 else 0,\n",
    "                        'Recall': perf_counts.get('TP', 0) / (perf_counts.get('TP', 0) + perf_counts.get('FN', 0)) if (perf_counts.get('TP', 0) + perf_counts.get('FN', 0)) > 0 else 0,\n",
    "                        'Accuracy': (perf_counts.get('TP', 0) + perf_counts.get('TN', 0)) / total if total > 0 else 0\n",
    "                    })\n",
    "      summary_data.append(summary_row)\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_path = os.path.join(output_dir, f\"evaluation_summary_{timestamp}.xlsx\")\n",
    "summary_df.to_excel(summary_path, index=False)\n",
    "print(f\"Saved comprehensive summary report to: {summary_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo_llm_312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
